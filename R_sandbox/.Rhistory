library(rvest)
library(httr)
library(dplyr)
library(stringr)
library(XML)
library(RCurl)
url_curr <- "https://ca.finance.yahoo.com/currencies"
url_curr <- "https://ca.finance.yahoo.com/currencies"
time_list <- c()
i = 0
i = 0
currency_price <- data.frame()
web_page_parsed <- htmlParse(GET(url_curr), encoding = "UTF-8")
table <- readHTMLTable(web_page_parsed)
table
typeof(table)
table <- table[[1]][,1:3]
table
price_list <-as.numeric(strsplit(toString(table$price),",")[[1]])[1:28]
price_list
price_list <-as.numeric(strsplit(toString(table$price),",")[[1]])[1:28]
price_list
knitr::opts_chunk$set(echo = TRUE)
# install.packages("rvest") # uncomment and run this line if rvest is not installed
# install.packages("dplyr") # uncomment and run this line if dplyr is not installed
# install.packages("stringr") # uncomment and run this line if stringr is not installed
# install.packages("stringi") # uncomment and run this line if stringi is not installed
library(rvest)
library(dplyr)
library(stringr)
library(stringi)
issues.root.html <- read_html("https://www.aeaweb.org/journals/aer/issues")
issues.root.html
issues.root.html %>%
html_nodes(".journal-preview")
issues.root.html %>%
html_nodes(".journal-preview")
issue.urls <- issues.root.html %>%
html_nodes(".journal-preview") %>% # . indicates a class name
html_nodes("a") # all links
print(issue.urls)
issues.root.html %>%
html_nodes(".journal-preview") %>% # . indicates a class name
html_nodes("a")
issues.root.html %>%
html_nodes(".journal-preview") %>% # . indicates a class name
html_nodes("a") %>% html_attr("href")
issue.urls <- issues.root.html %>%
html_nodes(".journal-preview") %>% # . indicates a class name
html_nodes("a") %>% # all links
html_attr("href") %>% # extract the links specified by `href`
as_data_frame() %>% # convert the links to a data frame object for filtering
print(issue.urls$value) # since issue.urls is a data.frame, need to extract $value
issue.urls <- paste0("https://www.aeaweb.org/", issue.urls$value)
print(head(issue.urls))
issues.root.html %>%
html_nodes(".journal-preview") %>% # . indicates a class name
html_nodes("a")
issues.root.html %>%
html_nodes(".journal-preview") %>% # . indicates a class name
html_nodes("a") %>% # all links
html_text
issue.years <- issues.root.html %>%
html_nodes(".journal-preview") %>% # . indicates a class name
html_nodes("a") %>% # all links
html_text %>% # extract the links specified by `href`
as_data_frame() %>% # convert the links to a data frame object for filtering
print(issue.years$value)
issue.years <- word(issue.years$value, start = 2, end = 2) # extract the second words
issue.years
issue.df <- data.frame(issue = issue.urls, year = issue.years)
print(head(issue.df))
issue.html <- read_html("https://www.aeaweb.org/issues/475")
issue.html %>%
html_nodes("section h1") %>% # access the html node with `section h1`
html_text() # filter the html data so that we have only string data
year.of.pub <- issue.html %>%
html_nodes("section h2") %>% # access the html node with `section h2`
html_text() # filter the html data so that we have only string data
print(year.of.pub) # this line contains volume #/month/year. Extract the very last word for year!
year.of.pub <- stri_extract_last_words(year.of.pub) # extract the last word
print(year.of.pub)
issue.html %>%
html_nodes("h3 a")
issue.html %>%
html_nodes("h3 a") %>% # access the html node with `h3 a`
html_text()
issue.html %>%
html_nodes("h3 a")
issue.html %>%
html_nodes("h3 a") %>% # access the html node with `h3 a`
html_attr("href")
article.urls <- paste0("https://www.aeaweb.org", article.urls)
article.urls <- issue.html %>%
html_nodes("h3 a") %>% # access the html node with `h3 a`
html_attr("href") # access what the node links to
print(article.urls)
article.urls <- paste0("https://www.aeaweb.org", article.urls)
article.urls
(read_html(article.urls[2])
)
read_html(article.urls[2])
article.urls[2]
read_html(article.urls[2]) %>%
html_nodes("section section")
read_html(article.urls[2]) %>%
html_nodes("section section") %>%
html_text()
GetAbstract <- function (article.url) {
(read_html(article.url) %>%
html_nodes("section section") %>%
html_text())[3]
}
GetArticleURLs <- function (issue.url) {
issue.html <- read_html(issue.url)
article.urls <- issue.html %>%
html_nodes("h3 a") %>% # access the html node with `h3 a`
html_attr("href") # access what the node links to
paste0("https://www.aeaweb.org", article.urls) # transform relative urls
}
abstracts <- sapply(GetArticleURLs("https://www.aeaweb.org/issues/475"), GetAbstract)
abstracts <- abstracts[2:length(abstracts)] # the first abstract is empty (Front Matter)
print(abstracts)
# install.packages("wordcloud") # uncomment and run this line if ggwordcloud is not installed
# install.packages("tm") # uncomment and run this line if tm is not installed
library(wordcloud)
library(tm)
text <- toString(abstracts) # combine all abstracts together
text
docs <- Corpus(VectorSource(text)) # construct a Corpus object based on text
inspect(docs)
docs <- tm_map(docs, toSpace, "\t") # replace \t with space
docs <- tm_map(docs, toSpace, "\n") # replace \n with space
toSpace <- content_transformer(function (x, pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "\t") # replace \t with space
docs <- tm_map(docs, toSpace, "\n") # replace \n with space
docs <- tm_map(docs, toSpace, "Abstract") # replace Abstract (title of each abstract) with space
inspect(docs)
docs <- tm_map(docs, removeWords, stopwords("en")) # remove stop words
docs <- tm_map(docs, content_transformer(tolower)) # lower case
docs <- tm_map(docs, removePunctuation) # remove punctuations
docs <- tm_map(docs, removeNumbers) # remove numbers
docs <- tm_map(docs, content_transformer(tolower)) # lower case
docs <- tm_map(docs, removeWords, stopwords("en")) # remove stop words
docs <- tm_map(docs, removePunctuation) # remove punctuations
docs <- tm_map(docs, removeNumbers) # remove numbers
docs <- tm_map(docs, stripWhitespace) # remove extra whitespaces
removeSpecialChars <- function(x) gsub("[^a-zA-Z0-9 ]"," ",x)
docs <- tm_map(docs, removeSpecialChars) # remove other special characters
inspect(docs)
docs.mat <- as.matrix(TermDocumentMatrix(docs)) # document matrix
docs.mat.sorted <- sort(rowSums(docs.mat),decreasing=TRUE) # sort them out
freq.table <- data.frame(word = names(docs.mat.sorted),freq=docs.mat.sorted) # construct a freq table
head(freq.table, 10)
set.seed(1234)
wordcloud(words = freq.table$word, freq = freq.table$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
library(ggmap)
geocode("10785 W. TWAIN AVE.,\nSUITE 210\n\nLAS VEGAS NV 89135\n", source= "dsk")
