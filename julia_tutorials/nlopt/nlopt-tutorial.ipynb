{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `NLopt` with autodiff and numerical gradients\n",
    "Presented by Chiyoung Ahn (https://github.com/chiyahn)\n",
    "\n",
    "This notebook demonstrates how nonlinear optimization problems in `NLopt` can be solved without the need of specifying analytic formulae for gradients by autodifferentiation or numerical gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "] add NLopt BenchmarkTools ForwardDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "using NLopt, BenchmarkTools, ForwardDiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `NLopt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear optimization without nonlinear constraints:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define the objective function `f` and the corresponding gradient `g!`. In `NLopt`, evaluation of `f` and execution of `g!` take place in the same time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fg! (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call g! then return f(x)\n",
    "function fg!(x::Vector, grad::Vector)\n",
    "    if length(grad) > 0 # gradient of f(x)\n",
    "        grad[1] = -2*x[1]*(x[1]^2 + x[2]^2)\n",
    "        grad[2] = -2*x[2]*(x[1]^2 + x[2]^2)\n",
    "    end\n",
    "    return -(x[1]^2 + x[2]^2)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the corresponding optimization problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Opt(:LD_LBFGS, 2) # 2 indicates the length of `x`\n",
    "lower_bounds!(opt, [-1.0, -1.0]) # find `x` above -2.0\n",
    "upper_bounds!(opt, [2.0, 2.0]) # find `x` below 2.0\n",
    "min_objective!(opt, fg!) # specifies that optimization problem is on minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and solve it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3.286 ms (21 allocations: 976 bytes)\n",
      "got -8.0 at [2.0, 2.0] after 2 iterations (returned SUCCESS)\n"
     ]
    }
   ],
   "source": [
    "(minf,minx,ret) = @btime optimize(opt, [1.0, 1.0])\n",
    "numevals = opt.numevals # the number of function evaluations\n",
    "println(\"got $minf at $minx after $numevals iterations (returned $ret)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear optimization with nonlinear constraints:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define `fg!` first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fg! (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function fg!(x::Vector, grad::Vector)\n",
    "    if length(grad) > 0 # gradient of f(x)\n",
    "        grad[1] = 0\n",
    "        grad[2] = 0.5/sqrt(x[2])\n",
    "    end\n",
    "    return sqrt(x[2]) # f(x)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the corresponding optimization problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Opt(:LD_SLSQP, 2) # 2 indicates the length of `x`\n",
    "lower_bounds!(opt, [-Inf, 0.]) # forces `x` to have a non-negative value\n",
    "min_objective!(opt, fg!) # specifies that optimization problem is on minimization\n",
    "xtol_rel!(opt,1e-4) # set a lower relative xtol for convergence criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly for constraint, where `constraint_f(x) <= 0` is imposed for all `x` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "constraint_fg! (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function constraint_fg!(x::Vector, grad::Vector, a, b)\n",
    "    if length(grad) > 0 # gradient of constraint_f(x)\n",
    "        grad[1] = 3a * (a*x[1] + b)^2\n",
    "        grad[2] = -1\n",
    "    end\n",
    "    (a*x[1] + b)^3 - x[2] # constraint_f(x); constraint_f(x) <= 0 is imposed\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `a` and `b` are added to allow variants of `constraint_f(x)` in a handy way. For instance, to impose\n",
    "```julia\n",
    "(2*x[1] + 0)^3 - x[2] <= 0\n",
    "```\n",
    "AND\n",
    "```julia\n",
    "(-1*x[1] + 1)^3 - x[2] <= 0\n",
    "```\n",
    "one can simply run the following two lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inequality_constraint!(opt, (x,g) -> constraint_fg!(x,g,2,0), 1e-8)\n",
    "inequality_constraint!(opt, (x,g) -> constraint_fg!(x,g,-1,1), 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ready to roll out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  14.115 Î¼s (242 allocations: 9.03 KiB)\n",
      "got 0.5443310539518157 at [0.333333, 0.296296] after 13 iterations (returned XTOL_REACHED)\n"
     ]
    }
   ],
   "source": [
    "(minf,minx,ret) = @btime optimize(opt, [1.234, 5.678])\n",
    "numevals = opt.numevals # the number of function evaluations\n",
    "println(\"got $minf at $minx after $numevals iterations (returned $ret)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `NLopt` without analytic formulae for gradients\n",
    "\n",
    "Suppose that `f` you want to optimize is written in a fairly complex form and you do not have an access to the analytic formula of the gradient of `f`. Here are some possible solutions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define `f` from the problem without nonlinear constraints above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function f(x::Vector)\n",
    "    return -(x[1]^2 + x[2]^2)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can define the corresponding numerical derivative `g!` and the `fg!` using forward-difference with some predetermined `EPS`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fg! (generic function with 1 method)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute gradient by forward-difference with EPS for epsilon\n",
    "function g!(G::Vector, x::Vector; EPS = 1e-8)\n",
    "    EPS = 1e-8\n",
    "    for i in 1:length(G)\n",
    "        x_forward = copy(x)\n",
    "        x_forward[i] += EPS\n",
    "        G[i] = (f(x_forward) - f(x)) / EPS\n",
    "    end\n",
    "end\n",
    "\n",
    "function fg!(x::Vector, grad::Vector)\n",
    "    if length(grad) > 0 # gradient of f(x)\n",
    "        g!(grad, x)\n",
    "    end\n",
    "    f(x)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3.284 ms (24 allocations: 1.31 KiB)\n",
      "got -8.0 at [2.0, 2.0] after 2 iterations (returned SUCCESS)\n"
     ]
    }
   ],
   "source": [
    "# define the optimization problem\n",
    "opt = Opt(:LD_LBFGS, 2) # 2 indicates the length of `x`\n",
    "lower_bounds!(opt, [-1.0, -1.0]) # find `x` above -2.0\n",
    "upper_bounds!(opt, [2.0, 2.0]) # find `x` below 2.0\n",
    "min_objective!(opt, fg!) # specifies that optimization problem is on minimization\n",
    "\n",
    "# solve the optimization problem\n",
    "(minf,minx,ret) = @btime optimize(opt, [1.0, 1.0])\n",
    "numevals = opt.numevals # the number of function evaluations\n",
    "println(\"got $minf at $minx after $numevals iterations (returned $ret)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or using automatic differentiation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fg! (generic function with 1 method)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute gradient by forward automatic differentiation\n",
    "function g!(G::Vector, x::Vector)\n",
    "    ForwardDiff.gradient!(G, f, x)\n",
    "end\n",
    "\n",
    "function fg!(x::Vector, grad::Vector)\n",
    "    if length(grad) > 0 # gradient of f(x)\n",
    "        g!(grad, x)\n",
    "    end\n",
    "    f(x)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3.294 ms (24 allocations: 1.28 KiB)\n",
      "got -8.0 at [2.0, 2.0] after 2 iterations (returned SUCCESS)\n"
     ]
    }
   ],
   "source": [
    "# define the optimization problem\n",
    "opt = Opt(:LD_LBFGS, 2) # 2 indicates the length of `x`\n",
    "lower_bounds!(opt, [-1.0, -1.0]) # find `x` above -2.0\n",
    "upper_bounds!(opt, [2.0, 2.0]) # find `x` below 2.0\n",
    "min_objective!(opt, fg!) # specifies that optimization problem is on minimization\n",
    "\n",
    "# solve the optimization problem\n",
    "(minf,minx,ret) = @btime optimize(opt, [1.0, 1.0])\n",
    "numevals = opt.numevals # the number of function evaluations\n",
    "println(\"got $minf at $minx after $numevals iterations (returned $ret)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration with `NLSolversBase`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(TODO)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.0",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
